<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VividFace: Real-Time and Realistic <br>Facial Expression Shadowing
 <br>for Humanoid Robots</title>
  <link rel="icon" type="image/x-icon" href="static/images/ameca_favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- TODO move to css file -->
  <style>
    .small-sup {
        font-size: 0.6em; /* Smaller size */
    }
    .institution-logo {
            margin-top: 20px; /* top margin */
            max-width: 175px; /* max width */
        }

    .image-container {
        display: flex; /* Use flexbox for layout */
        justify-content: center; /* Center images horizontally */
        gap: 20px; /* Space between images */
    }
    .image-container figure {
        text-align: center; /* Center the caption */
    }

    .image-container figcaption {
    margin-top: 8px; /* Adds space between the image and caption */

  }
  .content {
    max-width: 100%;    /* Set the maximum width for the caption text */
    margin: 0 auto;    /* Center the content */
    line-height: 1.5;  /* Improves readability */
}

    .image-container img {
        /* width: 459px; Set fixed width */
        /* height: 220px; Set fixed height */
        margin-top: 15px; /* top margin */
        flex-basis: 100%;
        object-fit: cover; /* Maintain aspect ratio */
    }

    .single-image {
        flex-basis: 100%; /* Take full width for the single image */
        margin-top: 20px; /* Space above the single image */
    }

    .video-container {
            display: flex; /* Use flexbox for layout */
            /* justify-content: space-between; Space between videos */
            /* margin: 20px;  Optional margin */
        }
    .video {
            position: relative; /* Position for watermark */
            width: 500px; /* Fixed width for the video */
            margin-right: 20px; /* Right margin for spacing */
            /* margin: 0 10px;  Optional spacing between videos */
    }
    
    .video:last-child {
        margin-right: 0; /* Remove margin for the last video */
    }
    
    video {
        width: 100%; /* Responsive width */
        height: auto; /* Maintain aspect ratio */
    }
    .watermark {
        position: absolute;
        top: 0px;
        left: 0px;
        /*bottom: 10px;  Position at bottom */
        /*right: 10px;  Position at right */
        background: rgba(255, 255, 255, 0.7); /* Semi-transparent background */
        padding: 5px; /* Padding for the watermark */
        font-size: 14px; /* Font size for the watermark */
        color: black; /* Text color */
        /*border-radius: 5px;  Rounded corners */
    }
  
</style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VividFace: Real-Time and Realistic <br>Facial Expression Shadowing
              <br> for Humanoid Robots</h1>
            <div class="column has-text-centered">
                    <div class="publication-links">
                   <!-- Github link -->
                   <span class="link-block">
                    <a href="https://github.com/pi3-141592653/VividFace" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mov"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <div class="content has-text-centered">
        <div class="content has-text-justified">
        A demonstration of <strong>VividFace</strong>. The humanoid robot
faithfully imitates the facial expressions of the human performer
in real time. The <strong>shadowing of subtle details</strong>, such
as frowning, gaze direction, and head pose enhances realism.
        </div>
        </div>
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
        <div class="image-container">
          <figure>
              <img src="static/images/realworld_examples7.png" alt="distraction", class=""single-image>
              <figcaption> 
                <div class="content has-text-centered">
                <div class="content has-text-justified">
                Real-world examples of humanoid robots performing realistic facial expression imitation.
              </div>
              </div>
              </figcaption>
          </figure>
          
        </div>

    </div>
    
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid
robots and affective human–robot interaction. Existing progress in humanoid facial expression imitation remains limited, often failing to achieve either real-time performance or realistic
expressiveness due to offline video-based inference designs and insufficient ability to capture and transfer subtle expression details.
To address these limitations, we present VividFace, a realtime and realistic facial expression shadowing system for humanoid
robots. An optimized imitation framework X2CNet++ enhances expressiveness by fine-tuning the human-to-humanoid facial motion transfer module and introducing a featureadaptation
training strategy for better alignment across different image sources. Real-time shadowing is further enabled by a video-stream-compatible inference pipeline and a streamlined 
workflow based on asynchronous I/O for efficient communication across devices. VividFace produces vivid humanoid faces by mimicking human facial expressions within 0.05 seconds, while
generalizing across diverse facial configurations. Extensive realworld demonstrations validate its practical utility.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- core images -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">System Overview</h2>
        <div class="image-container">
          <figure>
              <img src="static/images/systemoverview8.png" alt="project lead", class=""single-image>
              <figcaption>
                <figcaption>
                  <div class="content has-text-centered">
                    <div class="content has-text-justified">
                      An overview of the <b>VividFace</b> workflow. An RGB camera captures human facial expression dynamics (A), 
      and the image frames (each frame denoted by \(I_d\)) are streamed to the server and processed by the imitation framework, 
      which consists of the motion transfer module \(\mathcal{M}_1\) and the mapping network \(\mathcal{M}_2\). 
      The motion transfer module produces an intermediate expression representation 
      \(I_m = \mathcal{M}_1(I_d; f_s, x_{c,s})\) that integrates human motion with a virtual robot face. 
      The mapping network then predicts control values 
      \(\hat{\mathbf{y}} = \mathcal{M}_2(I_m)\), which are used to drive the physical robot to reproduce the expression (B). 
      The intermediate data flow for three example frames is visualized on the right (C).
                    </div>
                    
                  </div>
                </figcaption>
 
                </div>
              </figcaption>
          </figure>
        </div>

    </div>
    
  </div>
</section>

<section class="here is-small">

<div class="hero-body">
<div class="container is-max-desktop">
<h2 class="title is-3">Real-Time Video Demonstrations</h2>
<div class="video-container">
  <video autoplay muted loop playsinline controls>
    <source src="static/videos/alice.mov" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <video autoplay muted loop playsinline controls>
    <source src="static/videos/sogol.mov" type="video/mp4">
    Your browser does not support the video tag.
  </video>
</div>
<br>

<div class="video-container">
  <video autoplay muted loop playsinline controls>
    <source src="static/videos/proj.mov" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <video autoplay muted loop playsinline controls>
    <source src="static/videos/olivia.mov" type="video/mp4">
    Your browser does not support the video tag.
  </video>
</div>
</div>
</div>


</section>












<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
          
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
           
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">
           
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->





  <!-- <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
